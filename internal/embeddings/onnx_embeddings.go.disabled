package embeddings

import (
	"fmt"
	"io"
	"net/http"
	"os"
	"path/filepath"
	"sync"

	"github.com/dpshade/goscriptureapi/internal/config"
	"github.com/rs/zerolog/log"
	ort "github.com/yalue/onnxruntime_go"
	"github.com/sugarme/tokenizer"
)

// ONNXEmbeddingService implements the real EmbeddingGemma model using ONNX Runtime
type ONNXEmbeddingService struct {
	config     *config.Config
	session    *ort.Session
	tokenizer  *tokenizer.Tokenizer
	modelPath  string
	initialized bool
	mu         sync.RWMutex
}

// NewONNXEmbeddingService creates a new ONNX-based embedding service
func NewONNXEmbeddingService(cfg *config.Config) (*ONNXEmbeddingService, error) {
	service := &ONNXEmbeddingService{
		config: cfg,
	}

	// Initialize ONNX Runtime
	if err := ort.InitializeEnvironment(); err != nil {
		return nil, fmt.Errorf("failed to initialize ONNX runtime: %w", err)
	}

	return service, nil
}

// Initialize downloads and loads the model and tokenizer
func (s *ONNXEmbeddingService) Initialize() error {
	s.mu.Lock()
	defer s.mu.Unlock()

	if s.initialized {
		return nil
	}

	// Create models directory
	modelDir := filepath.Join(s.config.DataDir, "models")
	if err := os.MkdirAll(modelDir, 0755); err != nil {
		return fmt.Errorf("failed to create model directory: %w", err)
	}

	// Download model if needed
	s.modelPath = filepath.Join(modelDir, "model.onnx")
	if err := s.downloadModelIfNeeded(); err != nil {
		return fmt.Errorf("failed to download model: %w", err)
	}

	// Load the ONNX model
	if err := s.loadONNXModel(); err != nil {
		return fmt.Errorf("failed to load ONNX model: %w", err)
	}

	// Load the tokenizer
	if err := s.loadTokenizer(); err != nil {
		return fmt.Errorf("failed to load tokenizer: %w", err)
	}

	s.initialized = true
	log.Info().Msg("ONNX EmbeddingGemma model initialized successfully")
	return nil
}

// downloadModelIfNeeded downloads the ONNX model from Hugging Face if it doesn't exist
func (s *ONNXEmbeddingService) downloadModelIfNeeded() error {
	if _, err := os.Stat(s.modelPath); err == nil {
		log.Info().Str("path", s.modelPath).Msg("Model file already exists")
		return nil
	}

	// Download from Hugging Face
	modelURL := "https://huggingface.co/onnx-community/embeddinggemma-300m-ONNX/resolve/main/model.onnx"
	
	log.Info().Str("url", modelURL).Msg("Downloading EmbeddingGemma ONNX model...")

	resp, err := http.Get(modelURL)
	if err != nil {
		return fmt.Errorf("failed to download model: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return fmt.Errorf("failed to download model: HTTP %d", resp.StatusCode)
	}

	// Create the model file
	file, err := os.Create(s.modelPath)
	if err != nil {
		return fmt.Errorf("failed to create model file: %w", err)
	}
	defer file.Close()

	// Copy the response body to file
	_, err = io.Copy(file, resp.Body)
	if err != nil {
		return fmt.Errorf("failed to save model file: %w", err)
	}

	log.Info().Str("path", s.modelPath).Msg("Model downloaded successfully")
	return nil
}

// loadONNXModel loads the ONNX model using ONNX Runtime
func (s *ONNXEmbeddingService) loadONNXModel() error {
	sessionOptions, err := ort.NewSessionOptions()
	if err != nil {
		return fmt.Errorf("failed to create session options: %w", err)
	}
	defer sessionOptions.Destroy()

	// Enable CPU optimizations
	if err := sessionOptions.SetIntraOpNumThreads(4); err != nil {
		log.Warn().Err(err).Msg("Failed to set intra-op threads")
	}

	// Create session - ONNX runtime API might be different, let's try basic session first
	session, err := ort.NewSession(s.modelPath, sessionOptions)
	if err != nil {
		return fmt.Errorf("failed to create ONNX session: %w", err)
	}

	s.session = session
	log.Info().Msg("ONNX model loaded successfully")
	return nil
}

// loadTokenizer loads the tokenizer for the model
func (s *ONNXEmbeddingService) loadTokenizer() error {
	// For now, use a simple fallback tokenizer
	// The actual tokenizer implementation would need more complex setup
	return s.loadFallbackTokenizer()
}

// downloadTokenizerIfNeeded downloads the tokenizer configuration
func (s *ONNXEmbeddingService) downloadTokenizerIfNeeded(tokenizerPath string) error {
	if _, err := os.Stat(tokenizerPath); err == nil {
		return nil
	}

	tokenizerURL := "https://huggingface.co/onnx-community/embeddinggemma-300m-ONNX/raw/main/tokenizer.json"
	
	resp, err := http.Get(tokenizerURL)
	if err != nil {
		return err
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return fmt.Errorf("HTTP %d", resp.StatusCode)
	}

	file, err := os.Create(tokenizerPath)
	if err != nil {
		return err
	}
	defer file.Close()

	_, err = io.Copy(file, resp.Body)
	return err
}

// loadFallbackTokenizer loads a basic tokenizer as fallback
func (s *ONNXEmbeddingService) loadFallbackTokenizer() error {
	// Create a basic tokenizer for now
	// In a real implementation, we'd load the proper GemmaTokenizer
	tk := tokenizer.NewTokenizer()
	if tk == nil {
		return fmt.Errorf("failed to create basic tokenizer")
	}
	
	s.tokenizer = tk
	log.Info().Msg("Basic tokenizer loaded")
	return nil
}

// EmbedQuery generates embeddings for a search query
func (s *ONNXEmbeddingService) EmbedQuery(text string) ([]float32, error) {
	prefixedText := config.ModelConfig.QueryPrefix + text
	return s.embed(prefixedText)
}

// EmbedDocument generates embeddings for a document
func (s *ONNXEmbeddingService) EmbedDocument(text string) ([]float32, error) {
	prefixedText := config.ModelConfig.DocumentPrefix + text
	return s.embed(prefixedText)
}

// embed generates embeddings using the ONNX model
func (s *ONNXEmbeddingService) embed(text string) ([]float32, error) {
	s.mu.RLock()
	defer s.mu.RUnlock()

	if !s.initialized {
		return nil, fmt.Errorf("model not initialized")
	}

	// Tokenize the text
	encoding, err := s.tokenizer.EncodeSingle(text, true)
	if err != nil {
		return nil, fmt.Errorf("failed to tokenize text: %w", err)
	}

	inputIDs := encoding.GetIds()
	attentionMask := encoding.GetAttentionMask()

	// Convert to int64 for ONNX
	inputIDsInt64 := make([]int64, len(inputIDs))
	attentionMaskInt64 := make([]int64, len(attentionMask))
	
	for i, id := range inputIDs {
		inputIDsInt64[i] = int64(id)
	}
	for i, mask := range attentionMask {
		attentionMaskInt64[i] = int64(mask)
	}

	// Create ONNX tensors
	inputShape := ort.NewShape(1, int64(len(inputIDsInt64)))
	inputTensor, err := ort.NewTensor(inputShape, inputIDsInt64)
	if err != nil {
		return nil, fmt.Errorf("failed to create input tensor: %w", err)
	}
	defer inputTensor.Destroy()

	maskTensor, err := ort.NewTensor(inputShape, attentionMaskInt64)
	if err != nil {
		return nil, fmt.Errorf("failed to create attention mask tensor: %w", err)
	}
	defer maskTensor.Destroy()

	// Run inference
	outputs, err := s.session.Run([]ort.ArbitraryTensor{inputTensor, maskTensor})
	if err != nil {
		return nil, fmt.Errorf("failed to run inference: %w", err)
	}
	defer func() {
		for _, output := range outputs {
			output.Destroy()
		}
	}()

	// Extract embeddings
	if len(outputs) != 1 {
		return nil, fmt.Errorf("expected 1 output, got %d", len(outputs))
	}

	// Get the output tensor data
	outputData := outputs[0].GetData()
	embeddings, ok := outputData.([]float32)
	if !ok {
		return nil, fmt.Errorf("unexpected output type: %T", outputData)
	}

	// Truncate to 128 dimensions (Matryoshka)
	targetDim := config.ModelConfig.Dimensions
	if len(embeddings) < targetDim {
		return nil, fmt.Errorf("output has %d dimensions, expected at least %d", len(embeddings), targetDim)
	}

	result := make([]float32, targetDim)
	copy(result, embeddings[:targetDim])

	return result, nil
}

// Close cleans up resources
func (s *ONNXEmbeddingService) Close() error {
	s.mu.Lock()
	defer s.mu.Unlock()

	if s.session != nil {
		s.session.Destroy()
		s.session = nil
	}

	if s.tokenizer != nil {
		// Note: Basic tokenizer may not have Close method
		s.tokenizer = nil
	}

	ort.DestroyEnvironment()
	s.initialized = false

	return nil
}